# ğŸ¯ E2E Test Philosophy - Finding Bugs, Not Proving Perfection

## âš ï¸ CRITICAL PRINCIPLE

**×”×˜×¡×˜×™× ×‘×•×“×§×™× ××ª ×”×¤×œ×˜×¤×•×¨××” ×›×¤×™ ×©×”×™× - ×œ× ××ª××™××™× ××ª ×”×˜×¡×˜×™× ×›×“×™ ×©×™×¢×‘×¨×•!**

**Tests check the platform as-is - DO NOT modify tests to make them pass!**

---

## ğŸ” Our Goal: Find Real Bugs

### What We DO:
1. âœ… Write tests that simulate real user behavior
2. âœ… Test actual functionality as users would use it
3. âœ… If a test fails â†’ **FIX THE PLATFORM**, not the test
4. âœ… Verify error messages match actual behavior
5. âœ… Test edge cases that might reveal bugs
6. âœ… Test error handling for real error scenarios

### What We DON'T Do:
1. âŒ Modify tests to skip failing assertions
2. âŒ Add conditional logic to make tests pass
3. âŒ Ignore failures because "it's expected"
4. âŒ Change expectations to match broken behavior
5. âŒ Comment out failing tests
6. âŒ Use "might fail" logic (unless testing error handling)

---

## ğŸ“‹ Test Writing Guidelines

### âœ… Good Test Examples:

```typescript
// âœ… GOOD: Tests actual functionality
test("should prevent creating overlapping lines", async ({ page }) => {
  // Create first line
  await page.getByRole("button", { name: /×¦×•×¨ ×œ×™×™×Ÿ ×—×“×©/i }).click();
  await page.getByLabel(/×©× ×”×œ×™×™×Ÿ/i).fill("Existing Line");
  await page.getByLabel(/×™×•× ×©× ×™/i).check();
  await page.getByLabel(/×©×¢×ª ×”×ª×—×œ×”/i).fill("18:00");
  await page.getByLabel(/×©×¢×ª ×¡×™×•×/i).fill("22:00");
  await page.getByRole("button", { name: /×©××•×¨/i }).click();
  await expect(page.getByText(/× ×•×¦×¨ ×‘×”×¦×œ×—×”/i)).toBeVisible({ timeout: 5000 });
  
  // Try to create overlapping line
  await page.getByRole("button", { name: /×¦×•×¨ ×œ×™×™×Ÿ ×—×“×©/i }).click();
  await page.getByLabel(/×©× ×”×œ×™×™×Ÿ/i).fill("Overlapping Line");
  await page.getByLabel(/×™×•× ×©× ×™/i).check();
  await page.getByLabel(/×©×¢×ª ×”×ª×—×œ×”/i).fill("19:00"); // Overlaps
  await page.getByLabel(/×©×¢×ª ×¡×™×•×/i).fill("23:00");
  await page.getByRole("button", { name: /×©××•×¨/i }).click();
  
  // If this fails, there's a bug in collision detection!
  await expect(page.getByText(/×”×ª× ×’×©×•×ª|collision/i)).toBeVisible({ timeout: 5000 });
});
```

### âŒ Bad Test Examples:

```typescript
// âŒ BAD: Makes test pass even if feature is broken
test("should prevent creating overlapping lines", async ({ page }) => {
  // ... create lines ...
  
  // BAD: Conditional assertion
  const errorVisible = await page.getByText(/×”×ª× ×’×©×•×ª/i).isVisible();
  if (errorVisible) {
    await expect(page.getByText(/×”×ª× ×’×©×•×ª/i)).toBeVisible();
  }
  // This hides bugs!
});

// âŒ BAD: Ignores failures
test("should validate input", async ({ page }) => {
  // ... fill form ...
  
  // BAD: Comments out failing assertion
  // await expect(page.getByText(/× ×“×¨×©/i)).toBeVisible(); // Sometimes doesn't work
});

// âŒ BAD: Changes expectation to match broken behavior
test("should show error", async ({ page }) => {
  // ... trigger error ...
  
  // BAD: Accepts wrong error message because platform shows wrong message
  await expect(page.getByText(/some wrong message/i)).toBeVisible();
});
```

---

## ğŸ› When Tests Fail

### Process:
1. **Run the test** - See what fails
2. **Investigate** - Check if it's a test issue or platform bug
3. **Fix the platform** - If it's a real bug, fix it in the codebase
4. **Verify** - Run test again to confirm fix
5. **Document** - If it was a bug, document it in CHANGELOG

### Example Workflow:

```bash
# 1. Run tests
pnpm test:e2e

# 2. Test fails: "Expected collision error but got success"
# 3. Investigate: Check collision detection code
# 4. Found bug: Collision detection not checking overlapping times correctly
# 5. Fix: Update collision detection logic in checkCollisions.ts
# 6. Test again: Should pass now
# 7. Commit: "fix: collision detection for overlapping times"
```

---

## âœ… Test Quality Checklist

Before considering a test "done", verify:

- [ ] Test checks actual functionality (not mocked/conditional)
- [ ] Test would fail if feature is broken
- [ ] Test doesn't have conditional assertions that hide failures
- [ ] Test uses real user interactions (clicks, types, etc.)
- [ ] Test waits for actual responses (not arbitrary timeouts)
- [ ] Test verifies expected outcomes (success/error messages)
- [ ] Test covers edge cases (boundary conditions, empty states, etc.)

---

## ğŸ¯ Our Commitment

**If a test fails, we fix the platform - not the test.**

The tests are our quality gate. They should catch bugs, not hide them.

---

## ğŸ“ Examples of Platform Fixes (Not Test Fixes)

### Example 1: Collision Detection Bug

**Test Failure:**
```
Expected: Collision error message
Actual: Line created successfully (with overlapping time)
```

**Action:**
- âœ… Fix: Update `checkCollisions.ts` to properly detect overlaps
- âŒ Wrong: Modify test to accept success

### Example 2: Validation Bug

**Test Failure:**
```
Expected: "× ×“×¨×© ×©×" error
Actual: No error shown, form submits
```

**Action:**
- âœ… Fix: Add validation in `CreateLineDialog.tsx`
- âŒ Wrong: Remove assertion from test

### Example 3: UI State Bug

**Test Failure:**
```
Expected: Dialog closes after save
Actual: Dialog stays open, no feedback
```

**Action:**
- âœ… Fix: Update dialog close logic in component
- âŒ Wrong: Test for dialog being open after save

---

**Remember: Tests that always pass are useless. Tests that catch bugs are valuable.**

